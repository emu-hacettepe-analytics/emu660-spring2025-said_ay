[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Analytics Lab",
    "section": "",
    "text": "Hello!! My name is Said Ay\nThis is my personal webpage.\nPlease stay tuned to follow my works on data analytics, blog posts, and more.\n\n\n\n Back to top"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project X",
    "section": "",
    "text": "Welcome to my project page.\nKeep an eye on this space to stay updated with my project activities.\n(The titles below are provided as examples; please feel free to adjust them as necessary.)"
  },
  {
    "objectID": "project.html#data-source",
    "href": "project.html#data-source",
    "title": "Project X",
    "section": "2.1 Data Source",
    "text": "2.1 Data Source\nxxxxxx"
  },
  {
    "objectID": "project.html#general-information-about-data",
    "href": "project.html#general-information-about-data",
    "title": "Project X",
    "section": "2.2 General Information About Data",
    "text": "2.2 General Information About Data\nxxxxxx"
  },
  {
    "objectID": "project.html#reason-of-choice",
    "href": "project.html#reason-of-choice",
    "title": "Project X",
    "section": "2.3 Reason of Choice",
    "text": "2.3 Reason of Choice\nxxxxxx"
  },
  {
    "objectID": "project.html#preprocessing",
    "href": "project.html#preprocessing",
    "title": "Project X",
    "section": "2.4 Preprocessing",
    "text": "2.4 Preprocessing\nxxxxxx"
  },
  {
    "objectID": "project.html#exploratory-data-analysis",
    "href": "project.html#exploratory-data-analysis",
    "title": "Project X",
    "section": "3.1 Exploratory Data Analysis",
    "text": "3.1 Exploratory Data Analysis\nxxxxxx"
  },
  {
    "objectID": "project.html#trend-analysis",
    "href": "project.html#trend-analysis",
    "title": "Project X",
    "section": "3.2 Trend Analysis",
    "text": "3.2 Trend Analysis\nxxxxxx"
  },
  {
    "objectID": "project.html#model-fitting",
    "href": "project.html#model-fitting",
    "title": "Project X",
    "section": "3.3 Model Fitting",
    "text": "3.3 Model Fitting\nxxxxxx"
  },
  {
    "objectID": "project.html#results",
    "href": "project.html#results",
    "title": "Project X",
    "section": "3.4 Results",
    "text": "3.4 Results\nxxxxxx"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "My Assignments",
    "section": "",
    "text": "On this page, I showcase the assignment I conducted for the [term and year, e.g. Spring 2024-2025] EMU660 Decision Making with Analytics course.\nPlease use left menu to navigate through my assignments.\n\n\n\n Back to top",
    "crumbs": [
      "My Assignments"
    ]
  },
  {
    "objectID": "about.html#employements",
    "href": "about.html#employements",
    "title": "About Me",
    "section": "Employements",
    "text": "Employements\n\nFirm ASELSAN, position Project Engineer, year 2021-2024\nFirm ASELSAN, position PMO, year 2024-ongoing"
  },
  {
    "objectID": "about.html#internships",
    "href": "about.html#internships",
    "title": "About Me",
    "section": "Internships",
    "text": "Internships\n\nFirm BUROTIME, position Production Planning Intern, year 2019\nFirm TRT, position Training Department Intern, year 2020"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "This page is under construction.\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/assignments/assignment-1.html",
    "href": "docs/assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "1 + 1\n\n[1] 2\n\n\nMy first assignment has two parts."
  },
  {
    "objectID": "assignments/assignment-1.html",
    "href": "assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "My first assignment has two parts.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-2.html",
    "href": "assignments/assignment-2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Assignment 2\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#a",
    "href": "assignments/assignment-1.html#a",
    "title": "Assignment 1",
    "section": "(a)",
    "text": "(a)\nVeri Bilimi ve Endüstri Mühendisliği Üzerine Sohbetler -Kerem Demirtaş & Erdi Daşdemir\nKerem Demirtaş, mühendislik ve veri bilimi alanında geniş bir deneyime sahip bir profesyonel. Otoyendüstri bölümü mezunu olduktan sonra, veri bilimi ve yapay zeka gibi ileri düzey konularda kendini geliştirmiş ve kariyerinde önemli adımlar atmış. İlk olarak oyun sektöründe, Spiky Games’te veri bilimi ve fiyatlandırma üzerine çalışmalar yapmış. Burada, kullanıcı davranışlarını ve demografik özelliklerini analiz ederek, doğru zamanda doğru fiyat teklifleri sunmaya yönelik modeller geliştirmiş. Bu tür projeler, Kerem’in veri analitiği ve optimizasyon konularındaki bilgisini pekiştirmiş.\nSonraki yıllarda Intel’de çalışmaya başlamış ve burada da veri bilimi uygulamalarını daha ileriye taşıyarak, endüstriyel veri analitiği üzerinde yoğunlaşmış. Ancak, profesyonel yolculuğunda en ilginç dönüm noktalarından biri, Arizona State Üniversitesi’ne giderek doktoraya başlamasıydı. Hem araştırma görevlisi olarak çalışırken hem de öğretim üyeliği yaparak akademik kariyerine katkı sağlamış. Bu süreç, ona çeşitli disiplinlerden gelen farklı bakış açılarıyla veri bilimi projelerini nasıl şekillendirebileceği konusunda derinlemesine bir anlayış kazandırmış.\nKerem, yalnızca akademik dünyada değil, aynı zamanda endüstriyel sektörde de büyük bir etki yaratmış. 2020 yılı civarında Türkiye’ye döndükten sonra, Spiky Games ve Smartkivi gibi startuplarda çalıştıktan sonra, 2023 Aralık’ta Invent Ai’ye katıldı. Burada, perakende sektöründeki veri analizlerine dayalı yazılım çözümleri üretiyor. Invent Ai, şirketlerin envanter yönetiminden talep tahminlerine kadar birçok alanda veri bilimi ve yapay zekayı kullanarak operasyonel verimliliği artırmayı amaçlayan bir platform sunuyor. Kerem, burada özellikle dinamik fiyatlandırma ve talep tahminleme konularında önemli katkılar sağlıyor.\nKonuşmasında, veri biliminin yalnızca büyük veriyle değil, küçük veri setleriyle de etkin bir şekilde kullanılabileceğini belirtiyor. Özellikle, veri temizleme süreçleri ve model değerlendirme konularına dikkat çekiyor. Kendisinin de vurguladığı gibi, veri biliminde doğru problem tanımlaması ve doğru veri analizi yapmak, başarıyı getiren en önemli unsurlar arasında. Kerem, aynı zamanda yapay zeka ve makine öğrenmesinin her zaman belirli bir insan faktörüyle desteklenmesi gerektiğini, çünkü makinelerin her zaman doğru sonuçlar veremeyeceğini belirtiyor.\nOtonom araçlar ve trafik akışının optimizasyonu üzerine yaptığı çalışmalar da oldukça dikkat çekici. Bu projelerde, araçların birbirleriyle iletişim kurarak daha verimli bir trafik akışı oluşturması gibi yenilikçi fikirler üzerinde çalışıyor. Otonom araçlar için geliştirdiği modellerde, araçların birbirlerine yakın mesafede güvenli bir şekilde seyahat etmelerini sağlayan algoritmalar kullanıyor. Bu çalışmalar, sadece trafik güvenliğini artırmakla kalmıyor, aynı zamanda trafik yoğunluğunu da azaltmaya yönelik çözümler sunuyor.\nKerem’in kariyerinde veri bilimi, yapay zeka ve optimizasyon alanları arasında güçlü bir bağ var. Bu bağ sayesinde, farklı sektörlerdeki sorunları çözmek için veri bilimi ve yazılım mühendisliğini birleştirerek yaratıcı çözümler geliştirebiliyor. Hem akademik hem de endüstriyel alandaki tecrübeleri, ona veri biliminin gücünü ve potansiyelini tam anlamıyla gösterdi.\nBu yüzden, otonom araçlar, trafik yönetimi ve veri analitiği gibi karmaşık projelerde önemli bir lider olarak yer alıyor. Gelecekte, hem Türkiye’de hem de dünyada veri bilimi ve yapay zeka uygulamalarını daha da ileriye taşıma konusunda büyük planları var.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#b",
    "href": "assignments/assignment-1.html#b",
    "title": "Assignment 1",
    "section": "(b)",
    "text": "(b)\n\nlibrary(dslabs)\n\n# 1. Custom Summary Function\ncompute_stats &lt;- function(x) {\n  statistics &lt;- list(\n    mean = mean(x, na.rm = TRUE),\n    median = median(x, na.rm = TRUE),\n    variance = var(x, na.rm = TRUE),\n    IQR = IQR(x, na.rm = TRUE),\n    min = min(x, na.rm = TRUE),\n    max = max(x, na.rm = TRUE)\n  )\n  return(statistics)\n}\n\n# 2. Applying the Function Using a 'for' Loop on mtcars dataset\nfor (col in colnames(mtcars)) {\n  statistics &lt;- compute_stats(mtcars[[col]])\n  cat(\"\\nStatistics for\", col, \":\\n\")\n  print(statistics)\n}\n\n\nStatistics for mpg :\n$mean\n[1] 20.09062\n\n$median\n[1] 19.2\n\n$variance\n[1] 36.3241\n\n$IQR\n[1] 7.375\n\n$min\n[1] 10.4\n\n$max\n[1] 33.9\n\n\nStatistics for cyl :\n$mean\n[1] 6.1875\n\n$median\n[1] 6\n\n$variance\n[1] 3.189516\n\n$IQR\n[1] 4\n\n$min\n[1] 4\n\n$max\n[1] 8\n\n\nStatistics for disp :\n$mean\n[1] 230.7219\n\n$median\n[1] 196.3\n\n$variance\n[1] 15360.8\n\n$IQR\n[1] 205.175\n\n$min\n[1] 71.1\n\n$max\n[1] 472\n\n\nStatistics for hp :\n$mean\n[1] 146.6875\n\n$median\n[1] 123\n\n$variance\n[1] 4700.867\n\n$IQR\n[1] 83.5\n\n$min\n[1] 52\n\n$max\n[1] 335\n\n\nStatistics for drat :\n$mean\n[1] 3.596563\n\n$median\n[1] 3.695\n\n$variance\n[1] 0.2858814\n\n$IQR\n[1] 0.84\n\n$min\n[1] 2.76\n\n$max\n[1] 4.93\n\n\nStatistics for wt :\n$mean\n[1] 3.21725\n\n$median\n[1] 3.325\n\n$variance\n[1] 0.957379\n\n$IQR\n[1] 1.02875\n\n$min\n[1] 1.513\n\n$max\n[1] 5.424\n\n\nStatistics for qsec :\n$mean\n[1] 17.84875\n\n$median\n[1] 17.71\n\n$variance\n[1] 3.193166\n\n$IQR\n[1] 2.0075\n\n$min\n[1] 14.5\n\n$max\n[1] 22.9\n\n\nStatistics for vs :\n$mean\n[1] 0.4375\n\n$median\n[1] 0\n\n$variance\n[1] 0.2540323\n\n$IQR\n[1] 1\n\n$min\n[1] 0\n\n$max\n[1] 1\n\n\nStatistics for am :\n$mean\n[1] 0.40625\n\n$median\n[1] 0\n\n$variance\n[1] 0.2489919\n\n$IQR\n[1] 1\n\n$min\n[1] 0\n\n$max\n[1] 1\n\n\nStatistics for gear :\n$mean\n[1] 3.6875\n\n$median\n[1] 4\n\n$variance\n[1] 0.5443548\n\n$IQR\n[1] 1\n\n$min\n[1] 3\n\n$max\n[1] 5\n\n\nStatistics for carb :\n$mean\n[1] 2.8125\n\n$median\n[1] 2\n\n$variance\n[1] 2.608871\n\n$IQR\n[1] 2\n\n$min\n[1] 1\n\n$max\n[1] 8\n\n# 3. Applying the Function Using 'sapply' on mtcars dataset\nstatistics_sapply &lt;- sapply(mtcars, compute_stats)\nprint(statistics_sapply)\n\n         mpg      cyl      disp     hp       drat      wt       qsec    \nmean     20.09062 6.1875   230.7219 146.6875 3.596563  3.21725  17.84875\nmedian   19.2     6        196.3    123      3.695     3.325    17.71   \nvariance 36.3241  3.189516 15360.8  4700.867 0.2858814 0.957379 3.193166\nIQR      7.375    4        205.175  83.5     0.84      1.02875  2.0075  \nmin      10.4     4        71.1     52       2.76      1.513    14.5    \nmax      33.9     8        472      335      4.93      5.424    22.9    \n         vs        am        gear      carb    \nmean     0.4375    0.40625   3.6875    2.8125  \nmedian   0         0         4         2       \nvariance 0.2540323 0.2489919 0.5443548 2.608871\nIQR      1         1         1         2       \nmin      0         0         3         1       \nmax      1         1         5         8       \n\n# 4. Applying the Function Using 'apply' on mtcars dataset\nstatistics_apply &lt;- apply(mtcars, 2, compute_stats)\nprint(statistics_apply)\n\n$mpg\n$mpg$mean\n[1] 20.09062\n\n$mpg$median\n[1] 19.2\n\n$mpg$variance\n[1] 36.3241\n\n$mpg$IQR\n[1] 7.375\n\n$mpg$min\n[1] 10.4\n\n$mpg$max\n[1] 33.9\n\n\n$cyl\n$cyl$mean\n[1] 6.1875\n\n$cyl$median\n[1] 6\n\n$cyl$variance\n[1] 3.189516\n\n$cyl$IQR\n[1] 4\n\n$cyl$min\n[1] 4\n\n$cyl$max\n[1] 8\n\n\n$disp\n$disp$mean\n[1] 230.7219\n\n$disp$median\n[1] 196.3\n\n$disp$variance\n[1] 15360.8\n\n$disp$IQR\n[1] 205.175\n\n$disp$min\n[1] 71.1\n\n$disp$max\n[1] 472\n\n\n$hp\n$hp$mean\n[1] 146.6875\n\n$hp$median\n[1] 123\n\n$hp$variance\n[1] 4700.867\n\n$hp$IQR\n[1] 83.5\n\n$hp$min\n[1] 52\n\n$hp$max\n[1] 335\n\n\n$drat\n$drat$mean\n[1] 3.596563\n\n$drat$median\n[1] 3.695\n\n$drat$variance\n[1] 0.2858814\n\n$drat$IQR\n[1] 0.84\n\n$drat$min\n[1] 2.76\n\n$drat$max\n[1] 4.93\n\n\n$wt\n$wt$mean\n[1] 3.21725\n\n$wt$median\n[1] 3.325\n\n$wt$variance\n[1] 0.957379\n\n$wt$IQR\n[1] 1.02875\n\n$wt$min\n[1] 1.513\n\n$wt$max\n[1] 5.424\n\n\n$qsec\n$qsec$mean\n[1] 17.84875\n\n$qsec$median\n[1] 17.71\n\n$qsec$variance\n[1] 3.193166\n\n$qsec$IQR\n[1] 2.0075\n\n$qsec$min\n[1] 14.5\n\n$qsec$max\n[1] 22.9\n\n\n$vs\n$vs$mean\n[1] 0.4375\n\n$vs$median\n[1] 0\n\n$vs$variance\n[1] 0.2540323\n\n$vs$IQR\n[1] 1\n\n$vs$min\n[1] 0\n\n$vs$max\n[1] 1\n\n\n$am\n$am$mean\n[1] 0.40625\n\n$am$median\n[1] 0\n\n$am$variance\n[1] 0.2489919\n\n$am$IQR\n[1] 1\n\n$am$min\n[1] 0\n\n$am$max\n[1] 1\n\n\n$gear\n$gear$mean\n[1] 3.6875\n\n$gear$median\n[1] 4\n\n$gear$variance\n[1] 0.5443548\n\n$gear$IQR\n[1] 1\n\n$gear$min\n[1] 3\n\n$gear$max\n[1] 5\n\n\n$carb\n$carb$mean\n[1] 2.8125\n\n$carb$median\n[1] 2\n\n$carb$variance\n[1] 2.608871\n\n$carb$IQR\n[1] 2\n\n$carb$min\n[1] 1\n\n$carb$max\n[1] 8\n\n# 5. Load the 'na_example' dataset from the dslabs package\ndata(\"na_example\")\n\n# Check if na_example is a vector, which is the case for this dataset\nif (!is.vector(na_example)) {\n  stop(\"The dataset 'na_example' is not a vector!\")\n}\n\n# Display the contents of the dataset\nprint(na_example)\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n# Identify missing values\nna_positions &lt;- which(is.na(na_example))\n\n# Report the total count of NA values and their positions\ncat(\"\\nTotal number of NAs:\", length(na_positions), \"\\n\")\n\n\nTotal number of NAs: 145 \n\ncat(\"NA positions:\", \"\\n\")\n\nNA positions: \n\nprint(na_positions)\n\n  [1]  12  17  27  50  51  52  59  65  66  68  91 117 125 126 127 128 139 140\n [19] 141 142 153 158 168 169 172 179 189 190 203 205 207 208 212 214 237 241\n [37] 243 244 253 257 265 267 269 279 282 287 290 298 310 325 326 330 341 348\n [55] 361 374 392 395 397 407 410 437 443 446 461 468 470 490 496 505 527 536\n [73] 539 553 561 576 578 589 599 603 609 616 618 620 630 633 636 641 652 653\n [91] 666 667 668 677 680 687 691 695 701 726 734 735 736 745 746 747 748 751\n[109] 756 762 767 788 789 807 821 826 832 838 843 844 845 849 850 853 859 869\n[127] 887 892 893 906 909 911 918 924 925 939 943 950 962 965 966 968 979 990\n[145] 999\n\n# Compute mean and standard deviation before handling NAs\nmean_before &lt;- mean(na_example, na.rm = TRUE)\nsd_before &lt;- sd(na_example, na.rm = TRUE)\n\ncat(\"\\nMean before handling NAs:\", mean_before, \"\\n\")\n\n\nMean before handling NAs: 2.301754 \n\ncat(\"Standard deviation before handling NAs:\", sd_before, \"\\n\")\n\nStandard deviation before handling NAs: 1.22338 \n\n# Handling Missing Values\n\n# Version 1: Replace all NA values with the median of the non-missing values\nna_example_version1 &lt;- na_example\nna_example_version1[is.na(na_example_version1)] &lt;- median(na_example_version1, na.rm = TRUE)\n\n# Compute mean and standard deviation for Version 1\nmean_v1 &lt;- mean(na_example_version1, na.rm = TRUE)\nsd_v1 &lt;- sd(na_example_version1, na.rm = TRUE)\n\ncat(\"\\nMean after replacing NAs with median:\", mean_v1, \"\\n\")\n\n\nMean after replacing NAs with median: 2.258 \n\ncat(\"Standard deviation after replacing NAs with median:\", sd_v1, \"\\n\")\n\nStandard deviation after replacing NAs with median: 1.136102 \n\n# Version 2: Replace all NA values with randomly selected non-missing values from the dataset\n# Set seed for reproducibility\nset.seed(123)\n\n# Make a copy of the dataset\nna_example_version2 &lt;- na_example\n\n# Loop through the dataset and replace NAs with randomly selected non-missing values\nfor (i in 1:length(na_example_version2)) {\n  if (is.na(na_example_version2[i])) {\n    # Get non-missing values\n    non_missing_vals &lt;- na_example_version2[!is.na(na_example_version2)]\n    # Replace NA with a random non-missing value\n    na_example_version2[i] &lt;- sample(non_missing_vals, 1)\n  }\n}\n\n# Compute mean and standard deviation for Version 2 (after replacing NAs)\nmean_v2 &lt;- mean(na_example_version2, na.rm = TRUE)\nsd_v2 &lt;- sd(na_example_version2, na.rm = TRUE)\n\n# Print the results\ncat(\"\\nMean after replacing NAs with random non-missing values:\", mean_v2, \"\\n\")\n\n\nMean after replacing NAs with random non-missing values: 2.285 \n\ncat(\"Standard deviation after replacing NAs with random non-missing values:\", sd_v2, \"\\n\")\n\nStandard deviation after replacing NAs with random non-missing values: 1.206329 \n\n# Comparison of Mean and Standard Deviation:\ncat(\"\\nComparison of Mean and Standard Deviation:\")\n\n\nComparison of Mean and Standard Deviation:\n\ncat(\"\\nOriginal Dataset - Mean:\", mean_before, \"SD:\", sd_before)\n\n\nOriginal Dataset - Mean: 2.301754 SD: 1.22338\n\ncat(\"\\nVersion 1 - Mean:\", mean_v1, \"SD:\", sd_v1)\n\n\nVersion 1 - Mean: 2.258 SD: 1.136102\n\ncat(\"\\nVersion 2 - Mean:\", mean_v2, \"SD:\", sd_v2)\n\n\nVersion 2 - Mean: 2.285 SD: 1.206329\n\n\nWhen handling missing data, both methods—replacing missing values with the median and replacing them with a random non-missing value—have their own strengths and weaknesses. Replacing missing values with the median is a more conservative approach. It helps maintain the central tendency of the data without being influenced by outliers, making it a stable choice, especially when the data is skewed. However, this method can reduce variability because it tends to produce less extreme values, which may not fully capture the natural diversity in the data. On the other hand, replacing missing values with a random non-missing value preserves the variability and distribution of the data, which is useful when you want to retain the original data’s spread. This approach is particularly beneficial if the missing data is assumed to be missing at random, as it fills in missing values with data points that reflect the existing distribution. However, if the data is not missing at random, this method could introduce bias.\nIn general, replacing missing values with the median is a safer and simpler method that ensures stability, especially when you’re unsure about the underlying patterns of missingness. This method is often preferred because it minimizes the risk of distorting the data, particularly in cases where the missingness is random. However, if you believe the missing data follows a predictable pattern or is not missing completely at random, replacing it with a random value may be more appropriate to preserve the natural variability of the data. In this case, since the data is relatively small and we want to avoid making overly drastic assumptions, using the median is likely the more practical choice.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "about.html#cv-download",
    "href": "about.html#cv-download",
    "title": "About Me",
    "section": "CV Download",
    "text": "CV Download\nYou can download my CV by clicking the button below:\nDownload My CV"
  },
  {
    "objectID": "Proje/Proje.html",
    "href": "Proje/Proje.html",
    "title": "Temporal and Regional Analysis of Earthquake Activity in Turkey (1995–2023)",
    "section": "",
    "text": "Welcome to our project page.\nHere, you’ll find updates and insights from our ongoing exploration of earthquake patterns in Turkey."
  },
  {
    "objectID": "Proje/Proje.html#data-source",
    "href": "Proje/Proje.html#data-source",
    "title": "Temporal and Regional Analysis of Earthquake Activity in Turkey (1995–2023)",
    "section": "2.1 Data Source",
    "text": "2.1 Data Source\nThe dataset used in this project was obtained from the USGS Earthquake Catalog, a widely used open-access platform that provides detailed global seismic activity records. For this project, we filtered the data to include only earthquakes that occurred within the borders of Turkey — specifically between 26°–45° Eastern longitudes and 36°–42° Northern latitudes — between the years 1995 and 2023. Additionally, only events with a magnitude of 2.0 or greater were included to focus on perceptible and potentially damaging seismic activity.\nThe dataset was downloaded in .csv format and contains information such as:\n\nDate and Time of the earthquake\n\nMagnitude\n\nDepth (in kilometers)\n\nLatitude and Longitude\n\nPlace (a descriptive region name)\n\nAfter downloading, the data was imported into R for further exploration and preprocessing."
  },
  {
    "objectID": "Proje/Proje.html#general-information-about-data",
    "href": "Proje/Proje.html#general-information-about-data",
    "title": "Temporal and Regional Analysis of Earthquake Activity in Turkey (1995–2023)",
    "section": "2.2 General Information About Data",
    "text": "2.2 General Information About Data\nThe dataset consists of approximately 30,000 earthquake events recorded in Turkey over a 28-year period. Each row in the dataset corresponds to a unique seismic event, with detailed information such as the time it occurred, its geographical coordinates, how deep it originated, and how strong it was. Most earthquakes are naturally concentrated around Turkey’s well-known fault zones, such as the North Anatolian Fault in the north and the East Anatolian Fault in the southeast. This spatial clustering is something we aim to explore visually in the later sections of the project.\nHere’s a brief overview of the variables:\n\ntime: Timestamp of the earthquake (UTC), including date and hour\n\nlatitude & longitude: Geographic location of the event\n\ndepth: Depth below the surface in kilometers\n\nmag: Magnitude on the Richter scale\n\nplace: Descriptive label provided by USGS, such as nearby city or region\n\nOverall, The structure of the dataset is tidy and consistent, which makes it easier to analyze without extensive restructuring. Thanks to its clean structure, the dataset served as an effective and convenient entry point for our analysis."
  },
  {
    "objectID": "Proje/Proje.html#reason-of-choice",
    "href": "Proje/Proje.html#reason-of-choice",
    "title": "Temporal and Regional Analysis of Earthquake Activity in Turkey (1995–2023)",
    "section": "2.3 Reason of Choice",
    "text": "2.3 Reason of Choice\nThere are several reasons why we chose this specific dataset:\n\nFirst and foremost, earthquakes have always been a critical topic in Turkey, especially following the devastating seismic events in 1999 and more recently in 2023. Understanding patterns in historical data can offer valuable insights into risk mitigation.\nSecond, the dataset is both rich and clean, and doesn’t require extensive preprocessing to begin analysis. It offers a long temporal range (1995–2023), which makes it ideal for trend detection.\nLastly, We wanted a dataset that is directly relevant to public safety and urban planning in Turkey. The output of this study could contribute to further academic or policy-oriented discussions about seismic preparedness."
  },
  {
    "objectID": "Proje/Proje.html#preprocessing",
    "href": "Proje/Proje.html#preprocessing",
    "title": "Temporal and Regional Analysis of Earthquake Activity in Turkey (1995–2023)",
    "section": "2.4 Preprocessing",
    "text": "2.4 Preprocessing\nAfter importing the CSV file into R, several preprocessing steps were performed to prepare the dataset for analysis:\n\nDatetime Conversion: The time column was converted into proper POSIXct format to enable time-based analysis.\nFiltering: Earthquakes below magnitude 2.0 were excluded, since they are too weak to be of concern in a risk-focused study.\nRegion Extraction: The place column was parsed to extract province-level identifiers where possible.\nDerived Variables: A year column was created from the datetime to allow annual comparisons.\nMissing Values: The dataset contained minimal missing data, which were either filtered out or imputed where reasonable.\n\nThe cleaned dataset was saved in .RData format for reproducibility and faster processing in future sessions.\n\neq_data &lt;- read.csv(\"turkey_earthquake_data.csv\")\neq_data$DateTime &lt;- as.POSIXct(eq_data$time, format=\"%Y-%m-%dT%H:%M:%OSZ\", tz=\"UTC\")\neq_data &lt;- subset(eq_data, mag &gt;= 3.0)\neq_data$year &lt;- format(eq_data$DateTime, \"%Y\")\nsave(eq_data, file=\"eq_data.RData\")"
  },
  {
    "objectID": "Proje/Proje.html#exploratory-data-analysis",
    "href": "Proje/Proje.html#exploratory-data-analysis",
    "title": "Temporal and Regional Analysis of Earthquake Activity in Turkey (1995–2023)",
    "section": "3.1 Exploratory Data Analysis",
    "text": "3.1 Exploratory Data Analysis\nIn this section, we will conduct an exploratory analysis of the earthquake data to understand its basic structure and identify potential patterns or issues.\ndeneme deneme deneme denemexxx"
  },
  {
    "objectID": "Proje/Proje.html#trend-analysis",
    "href": "Proje/Proje.html#trend-analysis",
    "title": "Temporal and Regional Analysis of Earthquake Activity in Turkey (1995–2023)",
    "section": "3.2 Trend Analysis",
    "text": "3.2 Trend Analysis\nxxxxxx"
  },
  {
    "objectID": "Proje/Proje.html#model-fitting",
    "href": "Proje/Proje.html#model-fitting",
    "title": "Temporal and Regional Analysis of Earthquake Activity in Turkey (1995–2023)",
    "section": "3.3 Model Fitting",
    "text": "3.3 Model Fitting\nxxxxxx"
  },
  {
    "objectID": "Proje/Proje.html#results",
    "href": "Proje/Proje.html#results",
    "title": "Temporal and Regional Analysis of Earthquake Activity in Turkey (1995–2023)",
    "section": "3.4 Results",
    "text": "3.4 Results\nxxxxxx"
  }
]